# AUTOGENERATED! DO NOT EDIT! File to edit: 00_models.ipynb (unless otherwise specified).

__all__ = ['init_func', 'layer_types', 'extract_layer', 'FlexConvLayer', 'init_default', 'FlexUnetEncoder',
           'FlexUnetDecoder', 'SUNET']

# Cell
import torch.nn as nn
import torch
import types

# Cell
def _get_conv(ndim: int):
    "Get Convolution Layer of any dimension"
    assert 1 <= ndim <=3
    return getattr(nn, f'Conv{ndim}d')

def _get_bn(ndim: int):
    "Get BatchNorm Layer of any dimension"
    assert 1 <= ndim <=3
    return getattr(nn, f'BatchNorm{ndim}d')



# Cell
def init_func(m, func=nn.init.kaiming_normal_):
    "Initialize pytorch model `m` weights with `func`"
    if func and hasattr(m, 'weight'): func(m.weight)
    return m

# Cell
def layer_types(m):
    "returns list of pytorch models type"
    if isinstance(m, list): return list(map(type, m))
    return list(map(type, m.children()))

# Cell
def extract_layer(m, name=torch.nn.modules.Conv3d):
    res = []
    for child in m.children():
        for layer in child.modules():
            if(isinstance(layer,name)):
                res.append(layer)
    return res

# Cell
class FlexConvLayer(nn.Sequential):
    '''
      Create Flexible Convolution layer.

      This module allows to create 1, 2, or 3D convolutional layers containing (optional) activation function,
      batch normalization, upsampling or additional Pytorch Classes

      Parameters:
       \n`ni`: in_channels
       \n`nf`: out_channels
       \n`ks`: kernal_size
       \n`st`: stride
       \n`pd`: padding default is 1
       \n`ups`: adds Upsampling layer if `True`
       \n`sf`: scale factore if `ups` = True  upsampling layer
       \n`bn`: adds BatchNorm layer if `True`
       \n`ndim`: number of dimensions for layers e.g if 3 will create `nn.Conv3D` or `nn.BatchNorm3d`
       \n`func`: initiation function by default `nn.init.kaiming_normal_`
       \n`xtra`: adds any extra nn.Layers
       \n`act_fn`: activation function

       \nReturns:
       Sequential model containing specified Paramaters

       '''
    def __init__(self, ni: int, nf: int, ks: int=3, st:int=1, ndim: int=3, sf: int=2, pd: int=None, act_fn=None, bn: bool=False, ups: bool=False, xtra=None, func=nn.init.kaiming_normal_, **kwargs):
        layers = []
        if pd is None: pd = 1
        conv_l = _get_conv(ndim)(in_channels=ni, out_channels=nf,kernel_size =ks, stride=st, padding=pd, **kwargs)
        init_func(conv_l, func)
        layers.append(conv_l)
        if ups       : layers.insert(0, nn.Upsample(scale_factor=sf))
        if bn        : layers.append(_get_bn(ndim)(nf))
        if act_fn    : layers.append(act_fn())
        if xtra      : layers.append(xtra)
        super().__init__(*layers)



# Cell
def init_default(m, func=nn.init.kaiming_normal_):
    "Initialize `m` weights with `func` and set `bias` to 0."
    if func and hasattr(m, 'weight'): func(m.weight)
    with torch.no_grad():
        if getattr(m, 'bias', None) is not None: m.bias.fill_(0.)
    return m

# Cell
class FlexUnetEncoder(nn.Module):
    '''
    Creates flexible encoder for Unets.

    Provided convolution depth will generate unet encoder based on `FlexConvLayer`. During forward pass will also return `features` tensor, contaning stored features which will be used in decoder for  concatinating during  upsampling. Last element of `feature` is `x` which used to enter first layer in `decoder`

    Parameters:
    \n`ni`: in_channels
    \n`nf`: out_channels
    \n`ks`: kernal_size
    \n`st`: stride
    \n`pd`: padding default is 1
    \n`bn`: adds BatchNorm layer if `True`
    \n`act_fn`: activation function
    \n`conv_depth`: number of conv layers

    \nReturns:
    Sequential encoder, and feautre list

    '''

    def __init__(self, ni: int, nf: int, ks: int, st: int, pd: int, conv_depth: int, ndim: int=3,  act_fn=nn.ELU, **kwargs):
        super().__init__()
        nf = nf
        self.module_dict = nn.ModuleDict()
        self.module_dict['pass_n'] = FlexConvLayer(ni, nf, act_fn=act_fn, ndim=ndim, **kwargs)
        self.module_dict['save_n'] = FlexConvLayer(nf, nf, act_fn=act_fn, ndim=ndim, **kwargs)
        for i in range(conv_depth):
            self.module_dict[f'pass_{i}_k'] = nn.Sequential(FlexConvLayer(nf, nf, ks=ks-1, st=st + 1, act_fn=act_fn, pd=pd, ndim=ndim, **kwargs))
            self.module_dict[f'save_{i}']  = nn.Sequential(FlexConvLayer(nf, nf*2, ks=ks, st=st, act_fn=act_fn, ndim=ndim, **kwargs), )
            self.module_dict[f'pass_{i}'] = nn.Sequential(FlexConvLayer(nf*2, nf*2, ks=ks, st=st, act_fn=act_fn, ndim=ndim, **kwargs))
            nf *=2
    def forward(self, x):
        features = []
        for i in self.module_dict:
            x = self.module_dict[i](x)
            if  i.startswith('save'): features.append(x)

        features = features[:-1];
        features.append(x)
        return features


#RENAME everything

# Cell
class FlexUnetDecoder(nn.Module):
    '''
    Creates flexible decoder for Unets.

    This class will autmatically create decoder based on encoder paramaters. In forward pass, it will take features generated from FlexUnetEcnder and concatinate them on upsampled layers.

    Parameters:
    \n`nf`: out_channels for the first layer in encoder
    \n`ks`: kernal_size
    \n`st`: stride
    \n`pd`: padding default is 1
    \n`conv_depth`: number of conv layers
    \n`act_fn`: activation function by default its `nn.ELU`

    Returns:
    Decoder Model, and output of unet Model wchich should match input Dimensions
    '''
    def __init__(self, nf: int,  ks: int, st: int, pd: int, conv_depth: int, ndim=3,  act_fn=nn.ELU, **kwargs):
        super().__init__()
        nf = self._get_enc_filter(nf, conv_depth)
        self.module_dict = nn.ModuleDict()

        for i in range(conv_depth):
            self.module_dict[f'conc_{i}'] = nn.Sequential(FlexConvLayer(nf, nf//2, ks=ks, st=st, ups = True, act_fn=act_fn, pd=pd, ndim=ndim, **kwargs))
            self.module_dict[f'pass_{i}'] = nn.Sequential(FlexConvLayer(nf, nf//2, ks=ks, st=st, act_fn=act_fn, ndim=ndim, **kwargs),
                                                          FlexConvLayer(nf//2, nf//2, ks=ks, st=st, act_fn=act_fn, ndim=ndim, **kwargs))
            nf //=2

    def forward(self, features):
        #x replaced with features
        #rework this a bit
        x = features.pop()
        for i in self.module_dict:
            if i.startswith('conc'):
                  x = self.module_dict[i](x)
                  x = torch.cat([x,features.pop()],1)
            else: x = self.module_dict[i](x)
        return x

    @staticmethod
    def _get_enc_filter(nf, conv_depth):
        '''calculates number of in filters for decoder model given conv_depth and nf
        in the first conv layer in unet encoder'''
        nf = nf
        for i in range(conv_depth): nf *=2
        return nf

# Cell
class SUNET(nn.Module):
    '''
    Generates 1D, 2D or 3D Unet.

    Autmatically genrates UNET model based on user specified paramaters

    Parameters:
    \n`ni`: in_channels
    \n`nf`: out_channels
    \n`ks`: kernal_size
    \n`st`: stride
    \n`pd`: padding default is 1
    \n`ndim`: (1, 2, 3) 2D or 3D depending on dimensions
    \n`conv_depth`: number of conv layers
    \n **kwargs: see `FlexConvLayer` for generating flexible conv layers

    Returns:
    \n Unet Model
    '''
    def __init__(self, ni, nc, ks, st, pd, conv_depth, ndim, **kwargs):
        super().__init__()
        self.encoder = FlexUnetEncoder(ni, nc, ks, st, pd, conv_depth, ndim, **kwargs)
        self.decoder = FlexUnetDecoder(nc, ks, st, pd+1, conv_depth, ndim, **kwargs)
    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x
