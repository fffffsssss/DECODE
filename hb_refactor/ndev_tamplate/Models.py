# AUTOGENERATED! DO NOT EDIT! File to edit: 00_models.ipynb (unless otherwise specified).

__all__ = ['init_func', 'layer_types', 'extract_layer', 'FlexConvLayer', 'init_default', 'FlexUnetEncoder',
           'FlexUnetDecoder', 'SUNET']

# Cell
import torch.nn as nn
import torch
import types

# Cell
def _get_conv(ndim: int):
    "Get Convolution Layer of any dimension"
    assert 1 <= ndim <=3
    return getattr(nn, f'Conv{ndim}d')

def _get_bn(ndim: int):
    "Get BatchNorm Layer of any dimension"
    assert 1 <= ndim <=3
    return getattr(nn, f'BatchNorm{ndim}d')

# Cell
def init_func(m, func=nn.init.kaiming_normal_):
    "Initialize pytorch model `m` weights with `func`"
    if func and hasattr(m, 'weight'): func(m.weight)
    return m

# Cell
def layer_types(m):
    "returns list of pytorch models type"
    if isinstance(m, list): return list(map(type, m))
    return list(map(type, m.children()))

# Cell
def extract_layer(m, name=torch.nn.modules.Conv3d):
    res = []
    for child in m.children():
        for layer in child.modules():
            if(isinstance(layer,name)):
                res.append(layer)
    return res

# Cell
class FlexConvLayer(nn.Sequential):
    '''Create Flexible Conv Layers
       \n`ni`: in_channels
       \n`nf`: out_channels
       \n`ks`: kernal_size
       \n`st`: stride
       \n`pd`: padding default is 1
       \n`sf`: scale factore if for upsampling layer
       \n`bn`: adds BatchNorm layer if `True`
       \n`ups`: adds Upsampling layer if `True`
       \n`ndim`: number of dimensions for layers e.g if 3 will create `nn.Conv3D` or `nn.BatchNorm3d`
       \n`func`: initiation function by default `nn.init.kaiming_normal_`
       \n`xtra`: adds any extra nn.Layers
       \n`act_fn`: activation function'''
    def __init__(self, ni, nf,ks=3, st=1, ndim=3, sf=2, pd=None, act_fn=None, bn=False, xtra=None, ups=False, func=nn.init.kaiming_normal_, **kwargs):

        layers = []
        if pd is None: pd = 1
        conv_l = _get_conv(ndim)(in_channels=ni, out_channels=nf,kernel_size =ks, stride=st, padding=pd, **kwargs)
        init_func(conv_l, func)
        layers.append(conv_l)
        if ups       : layers.insert(0, nn.Upsample(scale_factor=sf))
        if bn        : layers.append(_get_bn(ndim)(nf))
        if act_fn    : layers.append(act_fn())
        if xtra      : layers.append(xtra)
        super().__init__(*layers)

# Cell
def init_default(m, func=nn.init.kaiming_normal_):
    "Initialize `m` weights with `func` and set `bias` to 0."
    if func and hasattr(m, 'weight'): func(m.weight)
    with torch.no_grad():
        if getattr(m, 'bias', None) is not None: m.bias.fill_(0.)
    return m

# Cell
class FlexUnetEncoder(nn.Module):
    '''Creates flexible encoder for Unets
       \n`ni`: in_channels
       \n`nf`: out_channels
       \n`ks`: kernal_size
       \n`st`: stride
       \n`pd`: padding default is 1
       \n`bn`: adds BatchNorm layer if `True`
       \n`act_fn`: activation function
       \n`conv_depth`: number of conv layers
       '''

    def __init__(self, ni, nf, ks, st, pd, conv_depth, ndim=3,  act_fn=nn.ELU, **kwargs):
        super().__init__()
        nf = nf
        self.module_dict = nn.ModuleDict()
        self.module_dict[f'pass_n'] = FlexConvLayer(ni, nf, act_fn=act_fn, ndim=ndim, **kwargs)
        self.module_dict[f'save_n'] = FlexConvLayer(nf, nf, act_fn=act_fn, ndim=ndim, **kwargs)
        for i in range(conv_depth):
            self.module_dict[f'pass_{i}_k'] = nn.Sequential(FlexConvLayer(nf, nf, ks=ks-1, st=st + 1, act_fn=act_fn, pd=pd, ndim=ndim, **kwargs))
            self.module_dict[f'save_{i}'] = nn.Sequential(FlexConvLayer(nf, nf*2, ks=ks, st=st, act_fn=act_fn, ndim=ndim, **kwargs), )
            self.module_dict[f'pass_{i}'] = nn.Sequential(FlexConvLayer(nf*2, nf*2, ks=ks, st=st, act_fn=act_fn, ndim=ndim, **kwargs))
            nf *=2
    def forward(self, x):
        features = []
        for i in self.module_dict:
            x = self.module_dict[i](x)
            if  i.startswith('save'): features.append(x)

        features = features[:-1];
        features.append(x)
        return features

# Cell
class FlexUnetDecoder(nn.Module):
    '''Creates flexible dencoder for Unets
       \n`nf`: out_channels
       \n`ks`: kernal_size
       \n`st`: stride
       \n`pd`: padding default is 1
       \n`conv_depth`: number of conv layers
       '''
    def __init__(self, nf,  ks, st, pd, conv_depth, ndim=3,  act_fn=nn.ELU, **kwargs):
        super().__init__()
        nf = self._get_enc_filter(nf, conv_depth)
        self.module_dict = nn.ModuleDict()

        for i in range(conv_depth):
            self.module_dict[f'conc_{i}'] = nn.Sequential(FlexConvLayer(nf, nf//2, ks=ks, st=st, ups = True, act_fn=act_fn, pd=pd, ndim=ndim, **kwargs))
            self.module_dict[f'pass_{i}'] = nn.Sequential(FlexConvLayer(nf, nf//2, ks=ks, st=st, act_fn=act_fn, ndim=ndim, **kwargs),
                                                          FlexConvLayer(nf//2, nf//2, ks=ks, st=st, act_fn=act_fn, ndim=ndim, **kwargs))
            nf //=2

    def forward(self, features):
        #x replaced with features
        #rework this a bit
        x = features.pop()
        for i in self.module_dict:
            if i.startswith('conc'):
                  x = self.module_dict[i](x)
                  x = torch.cat([x,features.pop()],1)
            else: x = self.module_dict[i](x)
        return x

    @staticmethod
    def _get_enc_filter(nf, conv_depth):
        nf = nf
        for i in range(conv_depth): nf *=2
        return nf

# Cell
class SUNET(nn.Module):
    '''General UNET for 2D or 3D
       \n`ni`: in_channels
       \n`nf`: out_channels
       \n`ks`: kernal_size
       \n`st`: stride
       \n`pd`: padding default is 1
       \n`ndim`: (2, 3) 2D or 3D depending on dimensions
       \n`conv_depth`: number of conv layers
    '''
    def __init__(self, ni, nc, ks, st, pd, conv_depth, ndim, **kwargs):
        super().__init__()
        self.encoder = FlexUnetEncoder(ni, nc, ks, st, pd, conv_depth, ndim, **kwargs)
        self.decoder = FlexUnetDecoder(nc, ks, st, pd+1, conv_depth, ndim, **kwargs)
    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x
